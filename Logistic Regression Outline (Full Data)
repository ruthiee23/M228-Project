library(ggplot2)
library(cowplot)
 
url <- "https://raw.githubusercontent.com/martuzaiu/Ovarian_Cancer_Project/main/ovariantotal.csv"
 
data <- read.csv(url, header=FALSE)
 
head(data) # you see data, but no column names
 
colnames(data) <- c(
  "AFP",
  "AG",
  "Age",
  "ALB",
  "ALP",
  "ALT",
  "AST",
  "BASO#",
  "BASO%",
  "BUN",
  "Ca",
  "CA125",
  "CA19-9",
  "CA72-4",
  "CEA",
  "CL",
  "CO2CP",
  "CREA",
  "DBIL",
  "EO#",
  "EO%",
  "GGT",
  "GLO",
  "GLU",
  "HCT",
  "HE4",
  "HGB",
  "IBIL",
  "K",
  "LYM#",
  "LYM%",
  "MCH",
  "MCV",
  "Menopause",
  "Mg",
  "MONO#",
  "MONO%",
  "MPV",
  "Na",
  "NEU",
  "PCT",
  "PDW",
  "PHOS",
  "PLT",
  "RBC",
  "RDW",
  "TBIL",
  "TP",
  "UA",
  "TYPE"
)
 
head(data) # now we have data and column names
 
str(data) # this shows that we need to tell R which columns contain factors
# it also shows us that there are some missing values. There are "?"s
# in the dataset. These are in the "ca" and "thal" columns...
 
## First, convert "?"s to NAs...
##data[data == "?"] <- NA
 
## Add factors for variables that are factors
data[data$menopause == 0,]$menopause <- "no menopause"
data[data$menopause == 1,]$TYPE <- "menopause"
data$menopause <- as.factor(data$menopause)
 
## This next line replaces 0 and 1 with "BENIGN" and "CANCER"
data$TYPE <- ifelse(test=data$TYPE == 0, yes="BENIGN", no="CANCER")
data$TYPE <- as.factor(data$TYPE) # Now convert to a factor
 
str(data) ## this shows that the correct columns are factors
 
## Now determine how many rows have "NA" (aka "Missing data"). If it's just
## a few, we can remove them from the dataset, otherwise we should consider
## imputing the values with a Random Forest or some other imputation method.
#nrow(data[is.na(data$ca) | is.na(data$thal),])
#data[is.na(data$ca) | is.na(data$thal),]
## so 6 of the 303 rows of data have missing values. This isn't a large
## percentage (2%), so we can just remove them from the dataset
## NOTE: This is different from when we did machine learning with
## Random Forests. When we did that, we imputed values.
#nrow(data)
#data <- data[!(is.na(data$ca) | is.na(data$thal)),]
#nrow(data)
 
#####################################
##
## Now we can do some quality control by making sure all of the factor
## levels are represented by people with and without cancer (TYPE)
##
## NOTE: We also want to exclude variables that only have 1 or 2 samples in
## a category since +/- one or two samples can have a large effect on the
## odds/log(odds)
##
##
#####################################
xtabs(~ TYPE + AFP, data=data)
xtabs(~ TYPE + AG, data=data)
xtabs(~ TYPE + Age, data=data)
xtabs(~ TYPE + restecg, data=data)
xtabs(~ TYPE + exang, data=data)
xtabs(~ TYPE + slope, data=data)
xtabs(~ TYPE + ca, data=data)
xtabs(~ TYPE + thal, data=data)
 
#####################################
##
## Now we are ready for some logistic regression. First we'll create a very
## simple model that uses sex to predict heart disease
##
#####################################
 
## let's start super simple and see if sex (female/male) is a good
## predictor...
## First, let's just look at the raw data...
xtabs(~ hd + sex, data=data)
#           sex
# hd         F   M
# Healthy    71  89
# Unhealthy  25 112
## Most of the females are healthy and most of the males are unhealthy.
## Being female is likely to decrease the odds in being unhealthy.
##    In other words, if a sample is female, the odds are against it that it
##    will be unhealthy
## Being male is likely to increase the odds in being unhealthy...
##    In other words, if a sample is male, the odds are for it being unhealthy
 
###########
##
## Now do the actual logistic regression
##
###########
 
logistic <- glm(hd ~ sex, data=data, family="binomial")
summary(logistic)
## (Intercept)  -1.0438     0.2326  -4.488 7.18e-06 ***
##   sexM        1.2737     0.2725   4.674 2.95e-06 ***
 
## Let's start by going through the first coefficient...
## (Intercept)  -1.0438     0.2326  -4.488 7.18e-06 ***
##
## The intercept is the log(odds) a female will be unhealthy. This is because
## female is the first factor in "sex" (the factors are ordered,
## alphabetically by default,"female", "male")
female.log.odds <- log(25 / 71)
female.log.odds
 
## Now let's look at the second coefficient...
##   sexM        1.2737     0.2725   4.674 2.95e-06 ***
##
## sexM is the log(odds ratio) that tells us that if a sample has sex=M, the
## odds of being unhealthy are, on a log scale, 1.27 times greater than if
## a sample has sex=F.
male.log.odds.ratio <- log((112 / 89) / (25/71))
male.log.odds.ratio
 
## Now calculate the overall "Pseudo R-squared" and its p-value
 
## NOTE: Since we are doing logistic regression...
## Null devaince = 2*(0 - LogLikelihood(null model))
##               = -2*LogLikihood(null model)
## Residual deviacne = 2*(0 - LogLikelihood(proposed model))
##                   = -2*LogLikelihood(proposed model)
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
 
## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null
 
## chi-square value = 2*(LL(Proposed) - LL(Null))
## p-value = 1 - pchisq(chi-square value, df = 2-1)
1 - pchisq(2*(ll.proposed - ll.null), df=1)
1 - pchisq((logistic$null.deviance - logistic$deviance), df=1)
 
## Lastly, let's  see what this logistic regression predicts, given
## that a patient is either female or male (and no other data about them).
predicted.data <- data.frame(
  probability.of.hd=logistic$fitted.values,
  sex=data$sex)
 
## We can plot the data...
ggplot(data=predicted.data, aes(x=sex, y=probability.of.hd)) +
  geom_point(aes(color=sex), size=5) +
  xlab("Sex") +
  ylab("Predicted probability of getting heart disease")
 
## Since there are only two probabilities (one for females and one for males),
## we can use a table to summarize the predicted probabilities.
xtabs(~ probability.of.hd + sex, data=predicted.data)
 
#####################################
##
## Now we will use all of the data available to predict heart disease
##
#####################################
 
logistic <- glm(hd ~ ., data=data, family="binomial")
summary(logistic)
 
## Now calculate the overall "Pseudo R-squared" and its p-value
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
 
## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null
 
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
 
## now we can plot the data
predicted.data <- data.frame(
  probability.of.hd=logistic$fitted.values,
  hd=data$hd)
 
predicted.data <- predicted.data[
  order(predicted.data$probability.of.hd, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
 
## Lastly, we can plot the predicted probabilities for each sample having
## heart disease and color by whether or not they actually had heart disease
ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) +
  geom_point(aes(color=hd), alpha=1, shape=4, stroke=2) +
  xlab("Index") +
  ylab("Predicted probability of getting heart disease")
 
ggsave("heart_disease_probabilities.pdf")
